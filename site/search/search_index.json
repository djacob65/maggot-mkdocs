{"config":{"lang":["en"],"separator":"[\\\\s\\\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Maggot","text":"<p>Foster good data management, with \"data sharing\u201d in mind</p> <p> <p> Sharing descriptive metadata is the first essential step towards the so-called \u201cOpen Data\u201d approach. With this in mind, the Maggot tool was specially designed to allow users to easily add descriptive metadata to datasets produced within a collective (research unit, platform, multi-partner project, etc.), thus promoting the sharing of metadata within this same collective and beyond.</p> <p></p> <p></p>"},{"location":"about/","title":"Maggot - About","text":""},{"location":"about/#background","title":"Background","text":""},{"location":"about/#motives","title":"Motives","text":"<ul> <li>Meet the challenges of organizing, documenting, storing and sharing data from a site, a project or a structure (unit, platform, etc.).</li> <li>Have visibility of what is produced within the collective: datasets, software, databases, images, sounds, videos, analyses, codes, etc.</li> <li>Fall within an open science quality approach for sharing and reproducibility.</li> <li>Promote FAIR (at least the Findable &amp; Accessible criteria) within the collective</li> <li>Raise awareness among newcomers and students about a better description of what they produce</li> </ul>"},{"location":"about/#state-of-need","title":"State of need","text":"<ul> <li>Implementing a data management plan imposes prerequisites such as the externalization of data to be preserved outside of users' disk space. This does not only concern published data but all data produced during the duration of a project. Above all, this outsourcing makes it possible to gather the data in one place and already constitutes a first-level backup. This becomes even more necessary when temporary agents (doctoral students, post-docs, interns, fixed-term contracts) are involved in data production.</li> <li>Consequently, the concern arises about the organization of these storage spaces. Should they be harmonized, i.e. impose good practices such as i) the naming of folders and files, ii) a folder structure (docs, data, scripts, etc.), iii) the use of README files, etc.</li> <li>At a minimum, using a README file seems the simplest and least restrictive. But then the question arises \u201cwhat to put in it\u201d? Templates can be offered to simplify their writing. But then the question arises of how to use them effectively when we want to find information? With what vocabulary?</li> </ul>"},{"location":"about/#proposed-approach","title":"Proposed approach","text":"<ul> <li>The two main ideas behind the tool are:<ul> <li>Make the storage space also become a data repository, ensuring that metadata gets to the data</li> <li>Be able to \u201ccapture\u201d the user\u2019s metadata as easily as possible by using their vocabulary</li> </ul> </li> <li>Concerning the first idea: \"Just\" place a metadata file (JSON format) describing the project data in each subdirectory, and then find the projects and/or data corresponding to specific criteria. The choice fell on the JSON format, very suitable for describing metadata, readable by both humans and machines.</li> <li> <p>Concerning the second idea: Given the diversity of the fields, the approach chosen is to be both the most flexible and the most pragmatic possible by allowing users to choose their own vocabulary (controlled or not) corresponding to the reality of their field and their activities. However, a good approach is as much as possible to use only controlled vocabulary, that is to say relevant and sufficient vocabulary used as a reference in the field concerned to allow users to describe a project and its context without having to add additional terms. To this end, the tool must allow users a progressive approach towards the adoption of standardized controlled vocabularies (thesauri or even ontologies).</p> </li> <li> <p>With the approach proposed by Maggot, initially there is no question of opening the data, but of managing metadata associated with the data on a storage space with a precise perimeter represented by the collective (unit, team, project , platform, \u2026). The main characteristic of the tool is, above all, to \u201ccapture\u201d the metadata as easily as possible according to a well-chosen metadata schema. However, the opening of data via their metadata must be a clearly stated objective within the framework of projects financed by public institutions (e.g Europe). Therefore if you have taken care to correctly define your metadata schema so that it is possible to make a correspondence using a mapping file with a data repository recognized by the international community, then you can easily \"push\" its metadata with the data without having re-enter anything.</p> </li> </ul>"},{"location":"about/#links","title":"Links","text":"<ul> <li>Source code on Github : inrae/pgd-mmdt</li> <li>Instance online : UMR 1322 BFP</li> </ul>"},{"location":"about/#designers-developers","title":"Designers / Developers","text":"<ul> <li> <p>Daniel Jacob (@UMR BFP) | CATI @PROSODIe</p> </li> <li> <p>Fran\u00e7ois Ehrenmann (@UMR BioGECO) | CATI @GEDEOP</p> </li> <li> <p>Philippe Chaumeil (@UMR BioGECO)</p> </li> </ul>"},{"location":"about/#contributors","title":"Contributors","text":"<ul> <li>Edouard Guitton (@INRAE Dept. SA, @Emerg'IN)</li> </ul>"},{"location":"configuration/","title":"Maggot - Configuration","text":""},{"location":"configuration/#terminology-configuration","title":"Terminology configuration","text":"<p>A single file (web/conf/config_terms.txt) contains all the terminology. The input and search interfaces are completely generated from this definition file, thus defining each of the fields, their input type (checkbox, dropbox, textbox, ...) and the associated controlled vocabulary (ontology and thesaurus by autocompletion, drop-down list according to a list of fixed terms). This is why a configuration and conversion step into JSON<sup>1</sup> format is essential in order to be able to configure all the other modules (example: creation of the MongoDB database schema when starting the application before filling it).</p> <p> </p> <ul> <li>Note : The step numbers shown in the figure above are mentioned in brackets in the text below.</li> </ul>"},{"location":"configuration/#tsv-to-json","title":"TSV to JSON","text":"<ul> <li> <p>This function is used to generate the terminology definition file in JSON<sup>1</sup> format (config_terms.json) and the corresponding JSON-Schema<sup>2</sup> file (maggot-schema.json) from a tabulated file (1). You can either create a terminology definition file in TSV format from scratch (see below to have more details), or extract the file from the current configuration (see JSON to TSV).</p> </li> <li> <p>Once the terminology definition file has been obtained (2), you can load it and press 'Submit'.</p> </li> <li> <p>Three files are generated (3 &amp; 5):</p> </li> <li>config_terms.json and maggot-schema.json : These files should be placed in the web/conf directory (3). A (re)start of the application must be done in full mode (4) (sh ./run fullstart)</li> <li>config_doc.txt (5) : This file serves as a template for the documentation of the metadata profile. You should edit it with a spreadsheet program, and fill in the description column (6). Then it is used to generate the documentation file in markdown format (see TSV to DOC).</li> </ul>"},{"location":"configuration/#tsv-to-doc","title":"TSV to DOC","text":"<ul> <li> <p>This function generates the markdown<sup>4</sup> documentation file (doc.md) from the template file (config_doc.txt) which is itself generated from the metadata definition file (config_terms.txt, cf TSV to JSON).</p> </li> <li> <p>Once the template file for the documentation (config_doc.txt) has been edited and documented (6) (see below to have more details), you can load it and press Submit button.</p> </li> <li> <p>The documentation file in markdown format (doc.md) is thus generated (7) and must be placed in the web/docs directory (8). Users will have access to this documentation file via the web interface, in the documentation section, heading \"Metadata\".</p> </li> </ul>"},{"location":"configuration/#json-to-tsv","title":"JSON to TSV","text":"<ul> <li>This function allows you to extract the terminology definition file in TSV format (config_terms.txt) from the current configuration. This allows you to start from this file, either to adapt your own metadata profile or simply to modify it slightly.</li> </ul> <ol> <li> <p>JavaScript Object Notation : format used to represent structured information\u00a0\u21a9\u21a9</p> </li> <li> <p>JSON-Schema: vocabulary that allows you to annotate and validate JSON documents.\u00a0\u21a9</p> </li> <li> <p>TSV: open text format representing tabular data as \"tab-separated values\". Each row corresponds to a table row and the cells in a row are separated by a tab.\u00a0\u21a9</p> </li> <li> <p>Markdown: Markdown is a lightweight markup language designed to provide easy-to-read and easy-to-write syntax. A Markdown document can be read as is without appearing to have been marked up or formatted with special instructions.\u00a0\u21a9</p> </li> </ol>"},{"location":"dictionaries/","title":"Maggot - Dictionaries","text":""},{"location":"dictionaries/#presentation","title":"Presentation","text":"<ul> <li>The use of dictionaries has no other purpose to facilitate the entry of metadata, entry which can be long and repetitive in generalist data warehouses (such as repository based on Dataverse).</li> <li>Dictionaries allow you to record multiple information necessary to define an entity, such as the names of people or even the funders. These information, once entered and saved in a file called a dictionary, can be subsequently associated with the corresponding entity. </li> <li>The dictionaries offered by default are: people, funders, data producers, as well as a vocabulary dictionary allowing you to mix ontologies and thesauri from several sources.<ul> <li>The following dictionaries : people (people), funders (grant) and data producers (producer) must not be changed in their format nor in their name because they are strongly linked with their internal use. </li> <li>On the other hand, the vocabulary dictionary (vocabulary) can be duplicated but while keeping its format (same columns and same layout).</li> </ul> </li> <li>To add a new dictionary, simply create a directory under web/cvlist then putting the files corresponding to the dictionary inside. Dictionaries will be automatically found by browsing this directory.</li> <li>Dictionary files are made using a simple spreadsheet then exported in TSV<sup>1</sup> format.</li> <li>Dictionaries are accessed through secure access limited to administrators allowing their editing. The login is by default 'admin'. You can add another account for consultation only using the following command:  <pre><code>sh ./run passwd &lt;user&gt;\n</code></pre></li> </ul>"},{"location":"dictionaries/#the-people-dictionary","title":"The people dictionary","text":"<ul> <li>Note : must not be changed in its format nor in its name</li> <li> <p>Like any dictionary, there must be 3 files (see below). Please note that the names of these files must always contain the name of the dictionary, i.e. same as the directory.  </p> </li> <li> <p>The format of the file containing the dictionary data (people.txt) is defined by another file (people_format.txt).</p> </li> </ul> <p> </p> <ul> <li>Thus, we know that the people dictionary must contain 5 columns (last name, first name, institution, ORCID number and email address) and that some fields are mandatory (last name, first name, institution) and others optional (ORCID number, email address).</li> <li>Each of the fields must respect a format specified by a regular expression in order to be accepted as valid.</li> <li>Optionally, you can connect an web API to each of the fields in order to make an entry by autocompletion from a remote register. Currently only ROR (Research Organization Registry) web API is possible but the mechanism is in place for new extensions.</li> <li>The third file, a very simple script written in JavaScript, defines the way to retrieve the list of names (here by containing the first and last name). Note that the name of the variable must always be identical to that of the dictionary. <pre><code>var people = [];\n// Each item in the 'people' list consists of the first two columns (0,1) separated by a space\nget_dictionary_values('people', merge=[0,' ',1]) </code></pre></li> <li> <p>Below, an example is given when modifying a record. When you click on the Institute field which is connected to the ROR web API, the drop-down list of reseach organizations that can correspond in the register appears, if there are any.  </p> </li> <li> <p>Note: It is possible to edit dictionaries, by adding an entry for example, and at the same time be able to immediately find this new entry in the metadata entry in the Maggot tool. Indeed each dictionary is reloaded into memory as soon as the corresponding input box is clicked.</p> </li> </ul> <p></p>"},{"location":"dictionaries/#other-dictionaries","title":"Other dictionaries","text":"<ul> <li> <p>Funders : The dictionary of the funders allows you to define the funding agency, project ID and its corresponding URL.</p> <ul> <li>Note : must not be changed in its format nor in its name  </li> </ul> </li> <li> <p>Producers : The dictionary of the data producers allows you to define their Institute and  project ID and their corresponding URL. Optionally, you can add the URL of the logo.</p> <ul> <li>Note : must not be changed in its format nor in its name  </li> </ul> </li> <li> <p>Vocabulary : Use this dictionary for mixing thesauri and ontologies in order to better target the entire controlled vocabulary of its field of application. Only the vocabulary is mandatory, the URL linked to an ontology or a thesaurus is optional. See Vocabulary section to learn the extent of the possibilities concerning vocabulary in Maggot.</p> <ul> <li>Note : can be duplicated but while keeping its format (same columns and same layout).  </li> </ul> </li> </ul> <p></p> <ol> <li> <p>TSV: open text format representing tabular data as \"tab-separated values\". Each row corresponds to a table row and the cells in a row are separated by a tab.\u00a0\u21a9</p> </li> </ol>"},{"location":"infrastructure/","title":"Maggot - Infrastructure","text":""},{"location":"infrastructure/#infrastructure-local-remote-or-mixed","title":"Infrastructure : Local, Remote or Mixed","text":"<p>The necessary Infrastructure involves 1) a machine running a Linux OS and 2) a dedicated storage space.</p> <p>1 - The machine will most often be of \"virtual\" type because more simpler to deploy, either locally (with VM providers such as VirtualBox, VMware Workstation or MS Hyper-V) or remotely (e.g VMware ESXi, Openstack: example of deployment). Moreover, the OS of your machine must allow you the deployment of docker containers. See for more details on \u201cWhat is Docker\u201d. The minimum characteristics of the VM are:  2 cpu, 2 Go RAM, 8 Go HD.</p> <p>2 - The dedicated storage space could be either in the local space of the VM, or in a remote place on the network.</p> <ul> <li>If the storage space is directly included in the VM, then tools like WinSCP or RcloneBrowser will allow you to easily transfer your files to the data space.</li> <li>If the storage space is your collective's NAS, you will need to make sure to open the SMB port on your network's firewall. If both VM and data storage are not in the same private network, it will probably also require installing the sofware layer corresponding to your corporate VPN on the VM so that it can access your NAS. See example successfully tested.</li> <li>If the storage space is in a data center (e.g. NextCloud, Google Drive), then you will need to install a tool such as rclone on your VM in order to be able to mount the storage space on the VM's disk space. See example successfully tested.</li> </ul> <p></p>"},{"location":"installation/","title":"Maggot - Installation","text":""},{"location":"installation/#install-on-your-linux-computer-or-linux-unix-server","title":"Install on your linux computer or linux / unix server","text":"<p>Requirements: The installation must be carried out on a (virtual) machine with  a recent Linux OS that support Docker (see Infrastructure)</p> <p></p>"},{"location":"installation/#retrieving-the-code","title":"Retrieving the code","text":"<p>Go to the destination directory of your choice then clone the repository and <code>cd</code> to your clone path:</p> <pre><code>git clone https://github.com/inrae/pgd-mmdt.git pgd-mmdt\ncd pgd-mmdt\n</code></pre> <p></p>"},{"location":"installation/#installation-of-docker-containers","title":"Installation of Docker containers","text":"<p>MAGGOT uses 3 Docker images for 3 distinct services:</p> <ul> <li>pgd-mmdt-db which hosts the mongoDB database</li> <li>pgd-mmdt-scan which scans the data and updates the contents of the database and the web interface</li> <li>pgd-mmdt-web which hosts the web server and the web interface pages</li> </ul> <p></p>"},{"location":"installation/#configuration","title":"Configuration","text":"<ul> <li>run : defines web port, root of the data directory (including for development)</li> <li>dockerdbpart/initialisation/setupdb-js.template : defines MongoDB settings</li> <li>dockerscanpart/scripts/config.py : defines MongoDB settings (dbserver, dbport, username, password)</li> <li>web/inc/config/mongodb.inc : defines MongoDB settings (dbserver, dbport, username, password)</li> <li>web/inc/config/config.inc : defines many of web parameters (modify only if necessary)</li> <li>web/inc/config/local.inc : defines the application parameters specific to the local installation (not erase when updating).</li> </ul> <p>Warning : You have to pay attention to put the same MongoDB settings in all the above configuration files. It is best not to change anything. It would have been preferable to put a single configuration file but this was not yet done given the different languages involved (bash, javascript, python, PHP). To be done!</p> <p>Note : If you want to run multiple instances, you will need to change in the run file, i) the container names, ii) the data path, iii) the port number and the MongoDB volume name, and also the MongoDB port in the different configuration files.</p> <p>The following two JSON files are defined by default but can be easily configured from the web interface. See the Terminology Definition section.</p> <ul> <li>web/conf/config_terms.json : define the terminology</li> <li>web/conf/maggot-schema.json : define the JSON schema used to validate metadata files.</li> </ul> <p></p>"},{"location":"installation/#commands","title":"Commands","text":"<p>The run shell script allows you to perform multiple actions by specifying an option :</p> <pre><code>cd pgd-mmdt\nsh ./run &lt;option&gt;\n</code></pre> <p>Options:</p> <ul> <li>build : Create the 3 Docker images namely pgd-mmdt-db, pgd-mmdt-scan and pgd-mmdt-web</li> <li>start : 1) Launch the 3 services by creating the Docker containers corresponding to the Docker images; 2) Create also the MongoDB volume.</li> <li>stop :  1) Remove all the 3 Docker containers; 2) Remove the MongoDB volume.</li> <li>initdb : Create and initialize the Mongo collection</li> <li>scan : Scan the data and update the contents of the database and the web interface</li> <li>fullstart : Perform the 3 actions start, initdb and scan</li> <li>restart : Perform the 2 actions stop then fullstart</li> <li>ps : Check that all containers are running correctly</li> <li>passwd &lt;user&gt;: Define the admin password if no user is specified, allowing you to copy the new configuration file on the server via the web interface (see configuration and to add entries in dictionaries. If a user is specified, the dictionary consultation will be authorized for this user.</li> </ul> <p></p>"},{"location":"installation/#starting-the-application","title":"Starting the application","text":"<ul> <li> <p>You must first build the 3 docker container images if this has not already been done, by :    <pre><code>sh ./run build\n</code></pre></p> </li> <li> <p>The application can be sequentially started :</p> <ul> <li>Starting the web interface  <pre><code>sh ./run start\n</code></pre></li> <li>Initialization of the MongoDB database  <pre><code>sh ./run initdb\n</code></pre></li> <li>Scanning the data directory for metadata files (META_XXXX.json)  <pre><code>sh ./run scan\n</code></pre></li> </ul> </li> <li> <p>You can also launch these 3 steps with a single command:    <pre><code>sh ./run fullstart\n</code></pre></p> </li> </ul> <p></p>"},{"location":"installation/#stoping-the-application","title":"Stoping the application","text":"<ul> <li>To stop the application :    <pre><code>sh ./run stop\n</code></pre></li> </ul>"},{"location":"installation/#architecture-diagram","title":"Architecture diagram","text":"<p> Note: See how to do proceed for configuration steps. </p> <p></p>"},{"location":"presentation/","title":"Maggot - Presentation","text":""},{"location":"presentation/#main-features-of-maggot","title":"Main features of Maggot","text":"<ol> <li>Documente with Metadata your datasets produced within a collective of people, thus making it possible :<ul> <li>to answer certain questions of the Data Management Plan (DMP) concerning the organization, documentation, storage and sharing of data in the data storage space, </li> <li>to meet certain data and metadata requirements, listed for example by the Open Research Europe in accordance with the FAIR principles.</li> </ul> </li> <li>Search datasets by their metadata<ul> <li>Indeed, the descriptive metadata thus produced can be associated with the corresponding data directly in the storage space then it is possible to perform a search on the metadata in order to find one or more sets of data. Only descriptive metadata is accessible by default.</li> </ul> </li> <li>Publish the metadata of datasets along with their data files into an Europe-approved repository</li> </ol> <p>See a short Presentation and Poster for a quick overview.</p>"},{"location":"presentation/#overview-of-the-different-stages-of-metadata-management","title":"Overview of the different stages of metadata management","text":"<p> Note: The step numbers indicated in the figure correspond to the different points developed below </p> <p>1 - First you must define all the metadata that will be used to describe your datasets.   It is possible to define all the metadata using a single file (in TSV format, therefore using a spreadsheet). The metadata proposed by default was mainly established according to the DDI (Data Documentation Initiative) metadata schema. This schema also largely corresponds to that adopted by the Dataverse software. This is a primordial step because both input and search interfaces are completely generated from these definition files, defining in this way each of the fields, their input type (checkbox, dropbox, textbox, ...) and also the associated Controlled Vocabulary (ontology and thesaurus by autocompletion, drop-down list according to a list of fixed terms). See the Terminology Definition section.  </p> <p>2 - Entering metadata will be greatly facilitated by the use of dictionaries.   The dictionaries offered by default are: people, funders, data producers, as well as a vocabulary dictionary allowing you to mix ontologies and thesauri from several sources. Each of these dictionaries allows you to enter a name or a main title by autocompletion. Then the information associated with it will be added during export (data deposit, JSON-LD format, OAI-PMH).  </p> <p>3 - The web interface for entering metadata is entirely built on the basis of definition files.    The metadata is thus distributed according to the different sections chosen, each constituting a tab. Mandatory fields are marked with a red star and must be documented in order to be able to generate the metadata file (see screenshot 1 &amp; screenshot 2). The entry of metadata governed by a controlled vocabulary (dictionary, thesaurus or ontology) is done by autocompletion. Once the mandatory fields (at least) and other recommended fields (the best) have been entered, the metadata file can be generated in JSON format.  </p> <p>4 - The file generated in JSON format must be placed in the storage space reserved for this purpose.    The role played by this metadata file can be seen as a README file adapted for machines, but also readable by humans. With an internal structure, it offers coherence and consistency of information that a simple README file with a completely free and therefore unstructured text format does not allow. Furthermore, the central idea is to use the storage space as a local data repository, so that the metadata should go to the data and not the other way around. If the filebrowser application is installed, then you can directly access the data space according to your access rights. See on github.  </p> <p>5 - A search of the datasets can thus be carried out on the basis of the metadata.    Indeed, all the JSON metadata files are scanned and parsed according to a fixed period (30 min) then loaded into a MongoDB database. This allows you to perform searches based on predefined metadata. The search form, in its compact shape on the left of the screen, is almost the same as the entry form (see a screenshot). Depending on the search criteria, a list of data sets is provided, with for each of them a link pointing to the detailed sheet.  </p> <p>6 - The detailed metadata sheet provides all the metadata divided by section.   Unfilled metadata does not appear by default. When a URL can be associated with information (ORCID, Ontology, web site, etc.), you can click on it to go to the corresponding link. Likewise, it is possible to follow the associated link on each of the resources. From this sheet, you can also export the metadata according to different schemata (Dataverse, Zenodo, JSON-LD). See screenshot 1 &amp; screenshot 2.  </p> <p>7 - Finally, once you have decided to publish your metadata with your data, you can choose the repository   that suits you (currently repositories based on Dataverse and Zenodo are supported).  </p> <p></p>"},{"location":"definitions/","title":"Terminology Definition File","text":""},{"location":"definitions/#configuration-flexibility","title":"Configuration flexibility","text":"<p>The Maggot tool offers great flexibility in configuration. It allows you to completely choose all the metadata you want to describe your data. You can base yourself on an existing metadata schema, invent your own schema<sup>1</sup> or, more pragmatically, mix one or more schemas by introducing some metadata specific to your field of application. However, keep in mind that if you want to add descriptive metadata to your data then a certain amount of information is expected. But a completely different use of the tool is possible, it's up to you.</p> <p>There are two levels of definition files as shown the figure below:</p> <p></p> <p>1 - The first level concerns the definition of terminology (metadata). Clearly, this category is more akin to application configuration files. They represent the heart of the application around which everything else is based. The input and search interfaces are completely generated from these definition files (especially the web/conf/config_terms.txt file), thus defining each of the fields, their input type (checkbox, dropbox, textbox, ...) and the associated controlled vocabulary (ontology and thesaurus by autocompletion, drop-down list according to a list of fixed terms). This is why a configuration and conversion step into JSON format is essential in order to be able to configure all the other modules.</p> <p>2 - The second level concerns the definitions of mapping to a differently structured metadata schema (e.g. Dataverse, JSON-LD, OAI-PMH). Simply place the definition files in the configuration directory (web/conf) for them to be taken into account, provided you have adjusted the configuration (web/inc/config/config.inc).</p> <p>All definition files are made using a simple spreadsheet then exported in TSV format. </p> <p>The list of definition files in Maggot are given below. All must be put under the directory web/conf. </p> <p>See an example on line : https://pmb-bordeaux.fr/maggot/config/view and the corresponding form based on these definition files.</p> <p></p> <ol> <li> <p>How to create a descriptive metadata plan:  https://sustainableheritagenetwork.org/system/files/atoms/file/How_to_Create_a_Descriptive_Metadata_Plan.pdf \u21a9</p> </li> </ol>"},{"location":"definitions/config_terms/","title":"Maggot - Terminlogy Definition","text":""},{"location":"definitions/config_terms/#example-of-a-terminlogy-definition-file","title":"Example of a Terminlogy Definition file","text":"Field Section Required Search ShortView Type features Label Predefined terms title definition Y N 1 textbox width=350px Short name fulltitle definition Y Y 2 textbox Full title subject definition Y Y checkbox open=0 Subject Agricultural Sciences,Arts and Humanities,Astronomy and Astrophysics,Business and Management,Chemistry,Computer and Information Science,Earth and Environmental Sciences,Engineering,Law,Mathematical Sciences,Medicine Health and Life Sciences,Physics,Social Sciences,Other description definition Y Y areabox rows=6,cols=30 Description of the dataset note definition N Y areabox rows=4,cols=30 Notes status status N Y 3 dropbox width=350px Status of the dataset Processed,In progress,Unprocessed access_rights status N Y 4 dropbox width=350px Access rights to data Public,Mixte,Private language status N Y checkbox open=0 Language Czech,Danish,Dutch,English,Finnish,French,German,Greek,Hungarian,Icelandic,Italian,Lithuanian,Norwegian,Romanian,Slovenian,Spanish,Swedish lifeCycleStep status N Y multiselect autocomplete=lifecycle,min=1 Life cycle step license status N Y textbox autocomplete=license,min=1 License datestart status N Y datebox width=350px Start of collection dateend status N Y datebox width=350px End of collection dmpid status N Y textbox DMP identifier contacts management Y Y multiselect autocomplete=people,min=1 Contacts authors management Y Y multiselect autocomplete=people,min=1 Authors collectors management N Y multiselect autocomplete=people,min=1 Data collectors curators management N Y multiselect autocomplete=people,min=1 Data curators members management N Y multiselect autocomplete=people,min=1 Project members leader management N Y multiselect autocomplete=people,min=1 Project leader wpleader management N Y multiselect autocomplete=people,min=1 WP leader depositor management N Y textbox Depositor producer management N Y multiselect autocomplete=producer,min=1 Producer grantNumbers management N Y multiselect autocomplete=grant,min=1 Grant Information kindOfData descriptors Y Y checkbox open=0 Kind of Data Audiovisual,Collection,Dataset,Event,Image,Interactive Resource,Model,Physical Object,Service,Software,Sound,Text,Workflow,Other keywords descriptors N Y multiselect autocomplete=bioportal,onto=EFO:JERM:EDAM:MS:NMR:NCIT:OBI:PO:PTO:AGRO:ECOCORE:IOBC:NCBITAXON Keywords topics descriptors N Y multiselect autocomplete=VOvocab Topic Classification dataOrigin descriptors N Y checkbox open=0 Data origin observational data,experimental data,survey data,analysis data,text corpus,simulation data,aggregate data,audiovisual corpus,computer code,Other experimentfactor descriptors N Y multiselect autocomplete=vocabulary,min=1 Experimental Factor measurement descriptors N Y multiselect autocomplete=vocabulary,min=1 Measurement type technology descriptors N Y multiselect autocomplete=vocabulary,min=1 Technology type publication_citation descriptors N Y areabox rows=5,cols=30 Publication - Citation publication_idtype descriptors N Y dropbox width=200px Publication - ID Type -,ark,arXiv,bibcode,doi,ean13,eissn,handle,isbn,issn,istc,lissn,lsid,pmid,purl,upc,url,urn publication_idnumber descriptors N Y textbox width=400px Publication - ID Number publication_url descriptors N Y textbox Publication - URL comment other N Y areabox rows=15, cols=30 Additional information"},{"location":"definitions/dataverse/","title":"Dataverse Definition File","text":"<ul> <li>Dataverse definition File<ul> <li>Example of Dataverse JSON file generated based on the definition file itself given as an example below.<ul> <li>Dataverse JSON of the FRIM dataset</li> </ul> </li> <li>Below an example of Dataverse definition file (TSV)  </li> </ul> </li> </ul>"},{"location":"definitions/json-ld/","title":"JSON-LD Definition File","text":"<ul> <li> <p>JSON-LD definition File</p> <ul> <li>Metadata schemas used to build the model proposed by default:<ul> <li>Schema.org, Bioschemas.org, Datacite, DDI-RDF, DubinCore, Dataverse</li> </ul> </li> <li>Example of JSON-LD file generated based on the definition file itself given as an example below.<ul> <li>JSON-LD file of the FRIM dataset</li> </ul> </li> <li>Below an example of JSON-LD definition file (TSV)  </li> </ul> </li> </ul> <p></p>"},{"location":"definitions/mapping/","title":"Mapping Definition File","text":"<ul> <li> <p>Mapping definition File</p> <ul> <li>The mapping file is used as indicated by its name to match a term chosen by the user during entry with another term from an ontology or a thesaurus and therefore to obtain a URL which will be used for referencing. It can be used for each conversion requiring such a mapping (e.g. to the Dataverse, Zenodo or JSON-LD format). </li> <li> <p>The role of this definition file is illustrated with the figure above  </p> </li> <li> <p>The file must have 5 columns with headers defined as follows:</p> <ul> <li>column 1 - CVname : name of the mapping entry</li> <li>column 2 - CVtype : type of the CV target (must be either bioportal or skosmos)</li> <li>column 3 - CVurl : URL of the corresponding web API</li> <li>column 4 - CVterm : name of the thesaurus or the ontology list separated by a comma</li> <li>column 5 - CVlang : the chosen language (mainly for thesauri) </li> </ul> </li> <li> <p>Below an example of Mapping definition file (TSV)</p> </li> </ul> </li> </ul> <p> </p>"},{"location":"definitions/oai-pmh/","title":"OAI-PMH Definition File","text":"<ul> <li> <p>OAI-PMH definition File</p> <ul> <li>Based on the Open Archives Initiative Protocol for Metadata Harvesting - Version 2</li> <li>Example of a OAI-PMH Data Provider Validation</li> <li>Example of OAI-PMH output for a dataset<ul> <li>FRIM dataset</li> </ul> </li> <li> <p>Example of OAI-PMH definition file (TSV)  </p> </li> <li> <p>Another example of OAI-PMH definition file (TSV) with identifers &amp; vocabulary mapping  </p> </li> </ul> </li> </ul>"},{"location":"definitions/terminology/","title":"Terminology","text":""},{"location":"definitions/terminology/#definition-of-terminology","title":"Definition of terminology","text":"<ul> <li> <p>There are two configuration (alias definition) files to set up.</p> <ul> <li>The terminology definition file (config_terms.txt) serving to describe all terminology used to define the metadata of a dataset.</li> <li>The terminology documentation file (config_doc.txt) serving to documente all terminology definitions.</li> </ul> </li> <li> <p>Each time there is a change in these two definition files, it is necessary to convert them so that they are taken into account by the application.</p> </li> </ul> <p>Terminology is the set of terms used to define the metadata of a dataset. A single file (web/conf/config_terms.txt) contains all the terminology. The input and search interfaces are completely generated from this definition file, thus defining each of the fields, their input type (checkbox, dropbox, textbox, ...) and the associated controlled vocabulary (ontology and thesaurus by autocompletion, drop-down list according to a list of fixed terms).  It is therefore necessary to create and fill it, according to the following scheme:</p> <ul> <li> <p>Terminology is organised in several sections. By default 6 sections are proposed:</p> <ul> <li>DEFINITION : section for defining the short name of projet.</li> <li>STATUS : section to indicate the status of the dataset and its retention period</li> <li>MANAGEMENT : section for entering the metadata relating to the management of the dataset (access rights, scientific and technical management, end-users, contracts, valorisation)</li> <li>DESCRIPTORS : section for entering metadata about the (biological) description of the dataset and the type of data it contains.</li> <li>OTHER : section allowing to enter miscellious information (protocols, comments, issues, ...)</li> <li>RESOURCES : section for entering metadata about some resources, i.e description of data included in the dataset. This section does not require any configuration a priori, it is added de facto.</li> </ul> </li> <li> <p>For each section, fields are then defined. These fields can be defined according to the way they will be entered via the web interface. There are 6 different types of input: check boxes (checkbox), drop lists (dropbox), single-line text boxes (textbox), single-line text boxes with an additional box for multiple selection from a catalog of terms (multiselect), date picker (datebox) and multi-line text boxes (areabox).</p> </li> </ul> <p> </p> <ul> <li>For two types (checkbox and dropbox), it is possible to define the values to be selected (predefined terms).</li> </ul>"},{"location":"definitions/terminology/#structure-of-the-terminology-definition-file-tsv","title":"Structure of the Terminology definition file (TSV)","text":"<p>The file must have 9 columns with headers defined as follows:</p> <ul> <li>column 1 - Field : shortname of the fields</li> <li>column 2 - Section : shortname ot the sections</li> <li>column 3 - Required : indicates if the field is mandatory ('Y') or not ('N')</li> <li>column 4 - Search : indicates if the field can be used as a criterion search ('Y') or not ('N')</li> <li>column 5 - Shortview : indicates with ordered numbers if the field serves for the overview table after the search (empty by default)</li> <li>column 6 - Type : indicates the way they will be entered via the web interface (possible values are: textbox, dropbox, checkbox, multiselect, datebox and areabox).</li> <li>column 7 - Features : dependings on the Type value, one can specifiy some specific features. If several features, they must be separated by a comma.<ul> <li>open=0 or open=1 (checkbox) :  indicates if the selection is opened or not. See Vocabulary.</li> <li>autocomplete=entity (textbox, checkbox &amp; multiselect) :  The entity.js file must be present under web/cvlist/entity/ if the entity is a dictionary otherwise it must be present under web/js/autocomplete. See Vocabulary.</li> <li>width=NNNpx (textbox, dropbox, datebox) : allows you to specify the width of the box. Usefull if you want put several fields in the same line. See note 1 below.</li> <li>row=NN and cols=NN (areabox) : allows you to specify the row and column size of the textarea.</li> </ul> </li> <li>column 8 - Label : Labels corresponding to the fields that will appear in the web interface</li> <li> <p>column 9 - Predefined terms : for fields defined with a type equal to 'checkbox' or 'dropbox', one can give a list of terms separated by a comma.</p> </li> <li> <p>Notes</p> <ul> <li>the fields will be displayed in the same order as in the file and by section. So if you want to specify several textboxes with particular sizes so that they are on the same line, they should belong to the same section and follow each other in the file in the same order.</li> <li>the title and description fields are mandatory but not necessarily in the same section.</li> </ul> </li> </ul> <p>Below an example of Terminology definition file (TSV)  </p> <p>Example of Maggot JSON file generated based on the same definition file</p> <ul> <li>Maggot JSON of the FRIM dataset</li> </ul> <p></p>"},{"location":"definitions/terminology/#structure-of-the-terminology-documentation-file-tsv","title":"Structure of the Terminology documentation file (TSV)","text":"<p>The documentation definition fileis used to have online help for each field (small icon placed next to each label on the form). So it should only be modified when a field is added or deleted, or moved to another section.</p> <p>The file must have 3 columns with headers defined as follows:</p> <ul> <li>column 1 - Type : The type of the element, namely 'section', 'field' or 'option'. An 'option' type must correspond to each of the options for a field corresponding to a drop-down list.</li> <li>column 2 - Name : Name of the element. The names of the sections, variables and drop-down options must be exactly the same as those specified in the terminology definition file.</li> <li>column 3 - Description : The description corresponding to the element, serving as much as possible to give indications on the information to be selected or entered, in order to remove possible ambiguities.</li> </ul> <p>Below an example of Terminology documentation file (TSV)  </p> <p></p>"},{"location":"definitions/vocabulary/","title":"Maggot - Vocabulary","text":""},{"location":"definitions/vocabulary/#vocabulary","title":"Vocabulary","text":"<ul> <li>In this section we expose the full extent of the possibilities concerning the vocabulary in Maggot.</li> <li>Choosing the type of vocabulary and how to enter it depends entirely on what you put in the terminology definition file. However, some approaches require a little technicality by writing small scripts based on JavaScript, but nothing too serious. You can always take an already ready-made script and modify only the part that concerns your focus.</li> </ul> <p>1 -  Vocabulary based on a list of terms fixed in advance (checbox with feature open=0)</p> <ul> <li>List of well-chosen and limited Control Vocabulary e.g according to a reference e.g. Data Document Initiative.</li> </ul> <p> </p> <p>2 - Vocabulary open for addition (checkbox with feature open=1)</p> <ul> <li>allows you to collect the desired Control Vocabulary (CV) from users. In order to initiate the list, you can put some terms in the predefined terms column of the terminology definition file.</li> </ul> <p> </p> <p>3 - Vocabulary based on a web API in a text field (textbox)</p> <ul> <li>The web API is defined in a JavaScript file with the same name as the assigned variable (here cities) and must present under web/js/autocomplete. For example, to enter a French city you can use the API geo.api.gouv.fr. See cities.js</li> </ul> <p> </p> <p>4 - Vocabulary based on a dictionary with multiple selection (multiselect) </p> <ul> <li>Dictionaries allow you to record multiple information necessary to define an entity, such as the names of people or even the funders. These information, once entered and saved in a file called a dictionary. Based on a very simple JavaScript retrieving the complete list of items included in the dictionary, thus creating a sort of internal API, we can fill a Maggot field by  autocompletion related to a search for these items.</li> <li>The JavaScript file must be named dico.js and be present under web/cvlist/dico/ where dico is the name of the dictionary. See for instance people.js</li> </ul> <p> </p> <p>5 - Vocabulary based on a SKOSMOS Thesaurus with multiple selection (multiselect) </p> <ul> <li>SKOSMOS is a web tool facilitating the posting of controlled vocabulary online in the form of a thesaurus according to the SKOS data model. It offers a navigation interface as well as a web API. A simple JavaScript allows you to easily connect this web API with a multiselect field.</li> <li>The JavaScript file must have the same name as the assigned variable (here VOvocab) and must present under web/js/autocomplete. See for instance VOvocab.js.</li> </ul> <p> </p> <p>6 - Vocabulary based on an OntoPortal with multiple selection (multiselect) </p> <ul> <li>Portals based on OntoPortal offer the wealth of ontologies according to several domains of application (e.g. BioPortal in the biomedical domain, AgroPortal in the domain of plants).</li> <li>No need of JavaScript file. The Bioportal Autocompletion widget has been implemented into Maggot. You have to only declare the ontology you want to use directly into the terminology definition file in order to easily connect this widget with a multiselect field.</li> </ul> <p> </p> <p></p>"},{"location":"definitions/zenodo/","title":"Zenodo Definition File","text":"<ul> <li> <p>Zenodo definition File</p> <ul> <li>Example of Zenodo JSON file generated based on the definition file itself given as an example below.<ul> <li>Zenodo JSON of the FRIM dataset</li> </ul> </li> <li>Below an example of Zenodo definition file (TSV)  </li> </ul> </li> </ul>"},{"location":"publish/","title":"Maggot - Publish Metadata","text":""},{"location":"publish/#publish-metadata","title":"Publish Metadata","text":"<ul> <li>Once we have decided to publish our metadata with possibly our data, we can choose the repository that suits us. Currently repositories based on Dataverse and Zenodo are supported, both being Europe-approved repositories.</li> <li> <p>Using an approach that might be called \u201cmachine-readable metadata,\u201d it is possible to populate metadata for a dataset into one of the proposed data repositories via its web API, provided that you have taken care to correctly define your metadata schema so that it is possible to make a correspondence with the chosen data repository using a mapping definition file.</p> </li> <li> <p>The principle is illustrated by the figure above.</p> </li> </ul> <p> </p> <ul> <li>We start from the Maggot JSON format metadata file generated from the web interface and based on the metadata profile defined by the terminology definition files. </li> <li>Then from a file defining the correspondence between the Maggot fields and those of the target repository, we can perform a conversion to the JSON format supported by the web API of the target repository.</li> <li>During the process we enrich the metadata with controlled vocabularies based either on dictionaries or on thesauri and/or ontologies. For the latter cases, we use the web APIs of these sources to perform the mapping (see the definition of mapping).</li> <li>Finally, to be able to carry out the transfer i.e. the submission to the target repository (we say \"push\" for short), we first need to connect to the repository in order to retrieve the key (the API token) authorizing us to submit the dataset. This obviously assumes that we have the privileges (creation/modification rights) to do so.</li> </ul>"},{"location":"publish/dataverse/","title":"Maggot - Publish into Dataverse","text":""},{"location":"publish/dataverse/#publish-into-dataverse","title":"Publish into Dataverse","text":"<ul> <li>Based on the Dataverse Native API</li> </ul> <p>1 - To submit metadata to a Dataverse repository, you must first select a dataset either from the drop-down list corresponding to the datasets listed on the data storage space or a metadata file from your local disk.</p> <p>2 - You then need to connect to the repository in order to retrieve the key (the  API token) authorizing you to submit the dataset. This obviously assumes that you have the privileges (creation/modification rights) to do so.</p> <p>3 - After choosing the repository URL, you must also specify on which dataverse collection you want to deposit the datasets. As previously, you must have write rights to this dataverse collection.</p> <p> </p> <p></p> <ul> <li>Then, all you have to do is click on 'Publish' to \"push\" the metadata to the repository. The figure below illustrates based on an example how the metadata is recorded in the repository as well as the Mapping corresponding to the fields linked to Controlled Vocabularies.</li> </ul> <p></p> <p></p>"},{"location":"publish/dataverse/#deposit-data-files","title":"Deposit data files","text":"<ul> <li> <p>If you also want to deposit data files at the same time as the metadata, you will need:</p> <ul> <li> <p>1 - declare the files to be deposited in the resources; these same files must also be present in the storage space.</p> </li> <li> <p>2 - create a so-called semaphore file (META_datafile_ok.txt); its sole presence, independently of its content (which may be empty) will authorize the transfer. Indeed, the creation of such a file guarantees that the user has actually write rights to the storage space corresponding to his dataset. This prevents someone else from publishing the data without having the right to do so. This mechanism also avoids having to manage user accounts on Maggot.</p> </li> </ul> </li> </ul> <p> </p> <p></p> <ul> <li>The figure below illustrates based on an example how data files appear on the repository with annotations corresponding to those created in Maggot.  </li> </ul> <p></p>"},{"location":"publish/zenodo/","title":"Maggot - Publish into Zenodo","text":""},{"location":"publish/zenodo/#publish-into-zenodo","title":"Publish into Zenodo","text":"<ul> <li>Based on the Zenodo REST-API</li> </ul> <p>1 - To submit metadata to a Zenodo repository, you must first select a dataset either from the drop-down list corresponding to the datasets listed on the data storage space or a metadata file from your local disk.</p> <p>2 - Unless you have previously saved your API token, you must create a new one and copy and paste it before validating it. Before validating, you must check the deposit:access and deposit:write boxes in order to obtain creation and modification rights with this token.</p> <p>3 - After choosing the repository URL, you can optionally choose a community to which the dataset will be linked. By default, you can leave empty this field.</p> <ul> <li>Warning : given the new changes introduced to the Zenodo validation process (October 2023), it seems that it is no longer possible to validate a community via API. Only a choice via the Zenodo web interface will allow you to do so in order to be validated later by the manager of this community.</li> </ul> <p> </p> <p></p>"},{"location":"publish/zenodo/#deposit-data-files","title":"Deposit data files","text":"<ul> <li> <p>If you also want to deposit data files at the same time as the metadata, you will need (see figure below)</p> <ul> <li> <p>1 - declare the files to be deposited in the resources (1) ; these same files must also be present in the storage space.</p> </li> <li> <p>2 - create a so-called semaphore file (META_datafile_ok.txt) (2); its sole presence, independently of its content will authorize the transfer. Indeed, the creation of such a file guarantees that the user has actually write rights to the storage space corresponding to his dataset. This prevents someone else from publishing the data without having the right to do so. This mechanism also avoids having to manage user accounts on Maggot.</p> </li> </ul> </li> <li> <p>Then, all you have to do is click on 'Publish' to \"push\" the metadata and data to the repository (3).</p> </li> <li> <p>After submission and if everything went well, a link to the deposit will be given to you (4).</p> </li> </ul> <p> </p> <p></p> <ul> <li>The figure below illustrates based on an example how the metadata and data is recorded in the repository.</li> </ul> <p> </p> <p></p>"}]}